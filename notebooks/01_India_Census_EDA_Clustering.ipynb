{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2d9f19",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis and Clustering on India Census 2011 Dataset\n",
    "\n",
    "## Project Overview\n",
    "This comprehensive data science project performs detailed exploratory data analysis (EDA) and clustering on the India Census 2011 dataset. The project aims to:\n",
    "- Understand demographic patterns across Indian states and districts\n",
    "- Identify key factors influencing literacy, population growth, and social development\n",
    "- Apply unsupervised learning techniques to group similar regions\n",
    "- Provide actionable insights for policy-making and resource allocation\n",
    "\n",
    "## Dataset Description\n",
    "The dataset contains demographic information from the 2011 Census of India, including:\n",
    "- Population statistics (total, male, female, children)\n",
    "- Literacy rates and educational status\n",
    "- Work force participation\n",
    "- Scheduled Caste and Scheduled Tribe populations\n",
    "- Geographic hierarchies (State, District, Sub-district levels)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d36dfeb",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735798f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Statistical analysis (using pandas and numpy instead of scipy)\n",
    "# Note: Using pandas and numpy functions instead of scipy to avoid memory issues\n",
    "\n",
    "# File handling\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üêç Python version: {sys.version}\")\n",
    "print(f\"üêº Pandas version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360c338",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d22307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = \"../data/2011-IndiaStateDistSbDist-0000.xlsx - Data.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(\"üìä Dataset loaded successfully!\")\n",
    "print(f\"üìè Dataset shape: {df.shape}\")\n",
    "print(f\"üìã Columns: {len(df.columns)}\")\n",
    "print(f\"üìÑ Rows: {len(df)}\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîç BASIC DATASET INFORMATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Data types:\\n{df.dtypes.value_counts()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìã FIRST 5 ROWS\")\n",
    "print(\"=\"*50)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9c26f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed information about the dataset\n",
    "print(\"üîç DETAILED DATASET INFORMATION\")\n",
    "print(\"=\"*50)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "missing_data = df.isnull().sum().sort_values(ascending=False)\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_data.index,\n",
    "    'Missing_Count': missing_data.values,\n",
    "    'Missing_Percentage': missing_percentage.values\n",
    "})\n",
    "\n",
    "print(f\"Total missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Columns with missing values: {len(missing_df[missing_df['Missing_Count'] > 0])}\")\n",
    "\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    print(\"\\nColumns with missing values:\")\n",
    "    print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b5d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the hierarchical structure\n",
    "print(\"üèõÔ∏è HIERARCHICAL STRUCTURE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"Unique values in key columns:\")\n",
    "hierarchical_cols = ['Level', 'TRU']\n",
    "for col in hierarchical_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: {df[col].unique()}\")\n",
    "\n",
    "# Analyze the administrative levels\n",
    "if 'Level' in df.columns:\n",
    "    level_counts = df['Level'].value_counts()\n",
    "    print(f\"\\nRecords by administrative level:\")\n",
    "    print(level_counts)\n",
    "\n",
    "# Analyze Total, Rural, Urban distribution\n",
    "if 'TRU' in df.columns:\n",
    "    tru_counts = df['TRU'].value_counts()\n",
    "    print(f\"\\nRecords by area type (TRU):\")\n",
    "    print(tru_counts)\n",
    "\n",
    "# Sample of different levels\n",
    "print(\"\\nüìã SAMPLE RECORDS BY LEVEL:\")\n",
    "if 'Level' in df.columns:\n",
    "    for level in df['Level'].unique()[:5]:  # Show first 5 levels\n",
    "        print(f\"\\n{level}:\")\n",
    "        sample = df[df['Level'] == level][['State', 'District', 'Name', 'TOT_P', 'P_LIT']].head(3)\n",
    "        if not sample.empty:\n",
    "            print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e3000a",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322eca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(\"üßπ DATA CLEANING PROCESS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Filter for state-level data only for meaningful analysis\n",
    "# We'll focus on state-level data for better insights\n",
    "print(\"üìä Filtering data for analysis...\")\n",
    "\n",
    "# Keep state-level total records for comprehensive analysis\n",
    "df_state = df_clean[(df_clean['Level'] == 'STATE') & (df_clean['TRU'] == 'Total')].copy()\n",
    "print(f\"State-level records: {len(df_state)}\")\n",
    "\n",
    "# Also create district-level data for more granular analysis\n",
    "df_district = df_clean[(df_clean['Level'] == 'DISTRICT') & (df_clean['TRU'] == 'Total')].copy()\n",
    "print(f\"District-level records: {len(df_district)}\")\n",
    "\n",
    "# Remove irrelevant columns for analysis\n",
    "cols_to_remove = ['State', 'District', 'Subdistt', 'Town/Village', 'Ward', 'EB', 'Level', 'TRU']\n",
    "\n",
    "# Check if columns exist before removing\n",
    "existing_cols_to_remove = [col for col in cols_to_remove if col in df_state.columns]\n",
    "print(f\"Removing columns: {existing_cols_to_remove}\")\n",
    "\n",
    "# Keep essential identification columns\n",
    "df_state_clean = df_state.drop(columns=existing_cols_to_remove, errors='ignore')\n",
    "df_district_clean = df_district.drop(columns=existing_cols_to_remove, errors='ignore')\n",
    "\n",
    "# Keep 'Name' column for identification\n",
    "if 'Name' in df_state.columns:\n",
    "    df_state_clean['State_Name'] = df_state['Name']\n",
    "if 'Name' in df_district.columns:\n",
    "    df_district_clean['District_Name'] = df_district['Name']\n",
    "\n",
    "print(f\"Cleaned state data shape: {df_state_clean.shape}\")\n",
    "print(f\"Cleaned district data shape: {df_district_clean.shape}\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"Duplicates in state data: {df_state_clean.duplicated().sum()}\")\n",
    "print(f\"Duplicates in district data: {df_district_clean.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "df_state_clean = df_state_clean.drop_duplicates()\n",
    "df_district_clean = df_district_clean.drop_duplicates()\n",
    "\n",
    "print(\"‚úÖ Data cleaning completed!\")\n",
    "print(f\"Final state data shape: {df_state_clean.shape}\")\n",
    "print(f\"Final district data shape: {df_district_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b567b64",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc66f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for State-level data\n",
    "print(\"üîß FEATURE ENGINEERING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def create_features(data):\n",
    "    \"\"\"Create meaningful features from census data\"\"\"\n",
    "    df_features = data.copy()\n",
    "    \n",
    "    # 1. Sex Ratio (Females per 1000 Males)\n",
    "    if 'TOT_F' in df_features.columns and 'TOT_M' in df_features.columns:\n",
    "        df_features['Sex_Ratio'] = (df_features['TOT_F'] / df_features['TOT_M']) * 1000\n",
    "    \n",
    "    # 2. Literacy Rate (%)\n",
    "    if 'P_LIT' in df_features.columns and 'TOT_P' in df_features.columns:\n",
    "        df_features['Literacy_Rate'] = (df_features['P_LIT'] / df_features['TOT_P']) * 100\n",
    "    \n",
    "    # 3. Male Literacy Rate (%)\n",
    "    if 'M_LIT' in df_features.columns and 'TOT_M' in df_features.columns:\n",
    "        df_features['Male_Literacy_Rate'] = (df_features['M_LIT'] / df_features['TOT_M']) * 100\n",
    "    \n",
    "    # 4. Female Literacy Rate (%)\n",
    "    if 'F_LIT' in df_features.columns and 'TOT_F' in df_features.columns:\n",
    "        df_features['Female_Literacy_Rate'] = (df_features['F_LIT'] / df_features['TOT_F']) * 100\n",
    "    \n",
    "    # 5. Child Population Ratio (0-6 years)\n",
    "    if 'P_06' in df_features.columns and 'TOT_P' in df_features.columns:\n",
    "        df_features['Child_Population_Ratio'] = (df_features['P_06'] / df_features['TOT_P']) * 100\n",
    "    \n",
    "    # 6. Work Participation Rate\n",
    "    if 'TOT_WORK_P' in df_features.columns and 'TOT_P' in df_features.columns:\n",
    "        df_features['Work_Participation_Rate'] = (df_features['TOT_WORK_P'] / df_features['TOT_P']) * 100\n",
    "    \n",
    "    # 7. Male Work Participation Rate\n",
    "    if 'TOT_WORK_M' in df_features.columns and 'TOT_M' in df_features.columns:\n",
    "        df_features['Male_Work_Rate'] = (df_features['TOT_WORK_M'] / df_features['TOT_M']) * 100\n",
    "    \n",
    "    # 8. Female Work Participation Rate\n",
    "    if 'TOT_WORK_F' in df_features.columns and 'TOT_F' in df_features.columns:\n",
    "        df_features['Female_Work_Rate'] = (df_features['TOT_WORK_F'] / df_features['TOT_F']) * 100\n",
    "    \n",
    "    # 9. SC Population Percentage\n",
    "    if 'P_SC' in df_features.columns and 'TOT_P' in df_features.columns:\n",
    "        df_features['SC_Population_Percent'] = (df_features['P_SC'] / df_features['TOT_P']) * 100\n",
    "    \n",
    "    # 10. ST Population Percentage\n",
    "    if 'P_ST' in df_features.columns and 'TOT_P' in df_features.columns:\n",
    "        df_features['ST_Population_Percent'] = (df_features['P_ST'] / df_features['TOT_P']) * 100\n",
    "    \n",
    "    # 11. Dependency Ratio (Non-working population dependency)\n",
    "    if 'NON_WORK_P' in df_features.columns and 'TOT_WORK_P' in df_features.columns:\n",
    "        df_features['Dependency_Ratio'] = df_features['NON_WORK_P'] / df_features['TOT_WORK_P']\n",
    "    \n",
    "    # 12. Gender Literacy Gap\n",
    "    if 'Male_Literacy_Rate' in df_features.columns and 'Female_Literacy_Rate' in df_features.columns:\n",
    "        df_features['Gender_Literacy_Gap'] = df_features['Male_Literacy_Rate'] - df_features['Female_Literacy_Rate']\n",
    "    \n",
    "    # 13. Gender Work Gap\n",
    "    if 'Male_Work_Rate' in df_features.columns and 'Female_Work_Rate' in df_features.columns:\n",
    "        df_features['Gender_Work_Gap'] = df_features['Male_Work_Rate'] - df_features['Female_Work_Rate']\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Apply feature engineering\n",
    "df_state_features = create_features(df_state_clean)\n",
    "df_district_features = create_features(df_district_clean)\n",
    "\n",
    "print(\"üìä Features created successfully!\")\n",
    "print(f\"State data shape after feature engineering: {df_state_features.shape}\")\n",
    "print(f\"District data shape after feature engineering: {df_district_features.shape}\")\n",
    "\n",
    "# Display newly created features\n",
    "new_features = ['Sex_Ratio', 'Literacy_Rate', 'Male_Literacy_Rate', 'Female_Literacy_Rate', \n",
    "                'Child_Population_Ratio', 'Work_Participation_Rate', 'Male_Work_Rate', \n",
    "                'Female_Work_Rate', 'SC_Population_Percent', 'ST_Population_Percent',\n",
    "                'Dependency_Ratio', 'Gender_Literacy_Gap', 'Gender_Work_Gap']\n",
    "\n",
    "existing_new_features = [col for col in new_features if col in df_state_features.columns]\n",
    "print(f\"\\nNew features created: {existing_new_features}\")\n",
    "\n",
    "# Display sample of new features\n",
    "print(\"\\nüìã SAMPLE OF NEW FEATURES (State Level):\")\n",
    "feature_sample = df_state_features[['State_Name'] + existing_new_features].head()\n",
    "print(feature_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affe2237",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb52125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Summary\n",
    "print(\"üìä STATISTICAL SUMMARY - KEY FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select key numerical features for analysis\n",
    "key_features = ['TOT_P', 'Sex_Ratio', 'Literacy_Rate', 'Male_Literacy_Rate', \n",
    "                'Female_Literacy_Rate', 'Child_Population_Ratio', 'Work_Participation_Rate',\n",
    "                'SC_Population_Percent', 'ST_Population_Percent']\n",
    "\n",
    "# Filter existing features\n",
    "existing_features = [col for col in key_features if col in df_state_features.columns]\n",
    "\n",
    "# Statistical summary\n",
    "stats_summary = df_state_features[existing_features].describe()\n",
    "print(stats_summary)\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\nüìà ADDITIONAL STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for feature in existing_features:\n",
    "    if feature in df_state_features.columns:\n",
    "        data = df_state_features[feature].dropna()\n",
    "        print(f\"\\n{feature}:\")\n",
    "        print(f\"  Skewness: {data.skew():.3f}\")\n",
    "        print(f\"  Kurtosis: {data.kurtosis():.3f}\")\n",
    "        print(f\"  Range: {data.max() - data.min():.2f}\")\n",
    "        print(f\"  Coefficient of Variation: {(data.std() / data.mean()) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8dda10",
   "metadata": {},
   "source": [
    "### 5.1 Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution Analysis - Histograms\n",
    "print(\"üìä DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Select features for distribution analysis\n",
    "dist_features = ['Literacy_Rate', 'Sex_Ratio', 'Work_Participation_Rate', 'Child_Population_Ratio']\n",
    "existing_dist_features = [col for col in dist_features if col in df_state_features.columns]\n",
    "\n",
    "# Create subplots for distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(existing_dist_features[:4]):\n",
    "    if feature in df_state_features.columns:\n",
    "        # Histogram with KDE\n",
    "        ax = axes[i]\n",
    "        data = df_state_features[feature].dropna()\n",
    "        \n",
    "        ax.hist(data, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax.axvline(data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {data.mean():.2f}')\n",
    "        ax.axvline(data.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {data.median():.2f}')\n",
    "        \n",
    "        ax.set_title(f'Distribution of {feature}', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel(feature)\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Distribution Analysis of Key Features', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary of distributions\n",
    "print(\"\\nüìà Distribution Statistics:\")\n",
    "for feature in existing_dist_features:\n",
    "    if feature in df_state_features.columns:\n",
    "        data = df_state_features[feature].dropna()\n",
    "        print(f\"{feature}: Mean={data.mean():.2f}, Std={data.std():.2f}, Skew={data.skew():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af47e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plots for Outlier Detection\n",
    "print(\"üì¶ OUTLIER DETECTION - BOX PLOTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create box plots for key features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(existing_dist_features[:4]):\n",
    "    if feature in df_state_features.columns:\n",
    "        ax = axes[i]\n",
    "        data = df_state_features[feature].dropna()\n",
    "        \n",
    "        bp = ax.boxplot(data, patch_artist=True)\n",
    "        bp['boxes'][0].set_facecolor('lightblue')\n",
    "        bp['boxes'][0].set_alpha(0.7)\n",
    "        \n",
    "        # Calculate and display outliers\n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "        \n",
    "        ax.set_title(f'{feature}\\\\nOutliers: {len(outliers)} ({len(outliers)/len(data)*100:.1f}%)', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Box Plot Analysis - Outlier Detection', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Print outlier details\n",
    "print(\"\\\\nüìä OUTLIER SUMMARY:\")\n",
    "for feature in existing_dist_features:\n",
    "    if feature in df_state_features.columns:\n",
    "        data = df_state_features[feature].dropna()\n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "        \n",
    "        print(f\"{feature}: {len(outliers)} outliers ({len(outliers)/len(data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ca5553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "print(\"üîó CORRELATION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Select numerical features for correlation analysis\n",
    "correlation_features = ['TOT_P', 'Sex_Ratio', 'Literacy_Rate', 'Male_Literacy_Rate', \n",
    "                        'Female_Literacy_Rate', 'Child_Population_Ratio', 'Work_Participation_Rate',\n",
    "                        'SC_Population_Percent', 'ST_Population_Percent', 'Gender_Literacy_Gap']\n",
    "\n",
    "# Filter existing features\n",
    "corr_features = [col for col in correlation_features if col in df_state_features.columns]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df_state_features[corr_features].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0, \n",
    "            mask=mask,\n",
    "            square=True, \n",
    "            fmt='.2f',\n",
    "            cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "plt.title('Correlation Heatmap - Census Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print highest correlations\n",
    "print(\"\\\\nüîç HIGHEST CORRELATIONS:\")\n",
    "# Get upper triangle of correlation matrix\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find highest correlations\n",
    "high_corr = []\n",
    "for col in upper_triangle.columns:\n",
    "    for row in upper_triangle.index:\n",
    "        corr_val = upper_triangle.loc[row, col]\n",
    "        if not pd.isna(corr_val) and abs(corr_val) > 0.5:\n",
    "            high_corr.append((row, col, corr_val))\n",
    "\n",
    "# Sort by absolute correlation value\n",
    "high_corr.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "for i, (var1, var2, corr) in enumerate(high_corr[:10]):  # Show top 10\n",
    "    print(f\"{i+1:2d}. {var1} <-> {var2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6778bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Analysis - Scatter Plots\n",
    "print(\"üìà BIVARIATE ANALYSIS - SCATTER PLOTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create interesting scatter plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Literacy Rate vs Sex Ratio\n",
    "ax1 = axes[0, 0]\n",
    "if 'Literacy_Rate' in df_state_features.columns and 'Sex_Ratio' in df_state_features.columns:\n",
    "    scatter = ax1.scatter(df_state_features['Sex_Ratio'], \n",
    "                         df_state_features['Literacy_Rate'],\n",
    "                         c=df_state_features['TOT_P'], \n",
    "                         cmap='viridis', \n",
    "                         alpha=0.7, \n",
    "                         s=100)\n",
    "    ax1.set_xlabel('Sex Ratio (Females per 1000 Males)')\n",
    "    ax1.set_ylabel('Literacy Rate (%)')\n",
    "    ax1.set_title('Literacy Rate vs Sex Ratio\\\\n(Color: Population Size)', fontweight='bold')\n",
    "    plt.colorbar(scatter, ax=ax1, label='Total Population')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Work Participation vs Literacy Rate\n",
    "ax2 = axes[0, 1]\n",
    "if 'Work_Participation_Rate' in df_state_features.columns and 'Literacy_Rate' in df_state_features.columns:\n",
    "    ax2.scatter(df_state_features['Literacy_Rate'], \n",
    "               df_state_features['Work_Participation_Rate'],\n",
    "               alpha=0.7, \n",
    "               color='coral', \n",
    "               s=100)\n",
    "    ax2.set_xlabel('Literacy Rate (%)')\n",
    "    ax2.set_ylabel('Work Participation Rate (%)')\n",
    "    ax2.set_title('Work Participation vs Literacy Rate', fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Gender Literacy Gap vs Overall Literacy\n",
    "ax3 = axes[1, 0]\n",
    "if 'Gender_Literacy_Gap' in df_state_features.columns and 'Literacy_Rate' in df_state_features.columns:\n",
    "    ax3.scatter(df_state_features['Literacy_Rate'], \n",
    "               df_state_features['Gender_Literacy_Gap'],\n",
    "               alpha=0.7, \n",
    "               color='lightgreen', \n",
    "               s=100)\n",
    "    ax3.set_xlabel('Overall Literacy Rate (%)')\n",
    "    ax3.set_ylabel('Gender Literacy Gap (Male - Female %)')\n",
    "    ax3.set_title('Gender Literacy Gap vs Overall Literacy', fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. SC Population vs Literacy Rate\n",
    "ax4 = axes[1, 1]\n",
    "if 'SC_Population_Percent' in df_state_features.columns and 'Literacy_Rate' in df_state_features.columns:\n",
    "    ax4.scatter(df_state_features['SC_Population_Percent'], \n",
    "               df_state_features['Literacy_Rate'],\n",
    "               alpha=0.7, \n",
    "               color='orange', \n",
    "               s=100)\n",
    "    ax4.set_xlabel('SC Population (%)')\n",
    "    ax4.set_ylabel('Literacy Rate (%)')\n",
    "    ax4.set_title('Literacy Rate vs SC Population %', fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Bivariate Analysis - Key Relationships', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Print correlation coefficients for the relationships\n",
    "print(\"\\\\nüìä CORRELATION COEFFICIENTS:\")\n",
    "relationships = [\n",
    "    ('Sex_Ratio', 'Literacy_Rate'),\n",
    "    ('Literacy_Rate', 'Work_Participation_Rate'),\n",
    "    ('Literacy_Rate', 'Gender_Literacy_Gap'),\n",
    "    ('SC_Population_Percent', 'Literacy_Rate')\n",
    "]\n",
    "\n",
    "for var1, var2 in relationships:\n",
    "    if var1 in df_state_features.columns and var2 in df_state_features.columns:\n",
    "        corr = df_state_features[var1].corr(df_state_features[var2])\n",
    "        print(f\"{var1} vs {var2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c5f300",
   "metadata": {},
   "source": [
    "## 6. Clustering Analysis (Unsupervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c666b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection for Clustering\n",
    "print(\"üéØ FEATURE SELECTION FOR CLUSTERING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Select key features for clustering analysis\n",
    "clustering_features = [\n",
    "    'Literacy_Rate', 'Sex_Ratio', 'Work_Participation_Rate', \n",
    "    'Child_Population_Ratio', 'SC_Population_Percent', 'ST_Population_Percent',\n",
    "    'Gender_Literacy_Gap', 'TOT_P'\n",
    "]\n",
    "\n",
    "# Filter existing features\n",
    "available_features = [col for col in clustering_features if col in df_state_features.columns]\n",
    "print(f\"Available features for clustering: {available_features}\")\n",
    "\n",
    "# Create dataset for clustering\n",
    "cluster_data = df_state_features[available_features + ['State_Name']].copy()\n",
    "\n",
    "# Remove any rows with missing values\n",
    "cluster_data_clean = cluster_data.dropna()\n",
    "print(f\"\\\\nData shape for clustering: {cluster_data_clean.shape}\")\n",
    "print(f\"States included in clustering: {len(cluster_data_clean)}\")\n",
    "\n",
    "# Prepare features (exclude State_Name)\n",
    "X = cluster_data_clean[available_features].copy()\n",
    "\n",
    "# Display basic statistics of clustering features\n",
    "print(\"\\\\nüìä CLUSTERING FEATURES STATISTICS:\")\n",
    "print(X.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Standardization\n",
    "print(\"‚öñÔ∏è DATA STANDARDIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "print(\"‚úÖ Features standardized successfully!\")\n",
    "print(f\"Scaled data shape: {X_scaled_df.shape}\")\n",
    "\n",
    "# Display before and after scaling comparison\n",
    "print(\"\\\\nüìä BEFORE AND AFTER SCALING COMPARISON:\")\n",
    "print(\"\\\\nBefore scaling (first 5 rows):\")\n",
    "print(X.head().round(2))\n",
    "\n",
    "print(\"\\\\nAfter scaling (first 5 rows):\")\n",
    "print(X_scaled_df.head().round(2))\n",
    "\n",
    "# Verify standardization\n",
    "print(\"\\\\nüîç STANDARDIZATION VERIFICATION:\")\n",
    "print(\"Mean values after scaling (should be ~0):\")\n",
    "print(X_scaled_df.mean().round(6))\n",
    "print(\"\\\\nStandard deviation after scaling (should be ~1):\")\n",
    "print(X_scaled_df.std().round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Optimal Number of Clusters\n",
    "print(\"üîç DETERMINING OPTIMAL NUMBER OF CLUSTERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Range of clusters to test\n",
    "k_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "print(\"Testing different numbers of clusters...\")\n",
    "\n",
    "# Calculate metrics for different k values\n",
    "for k in k_range:\n",
    "    # KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Calculate inertia (within-cluster sum of squares)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    sil_score = silhouette_score(X_scaled, cluster_labels)\n",
    "    silhouette_scores.append(sil_score)\n",
    "    \n",
    "    print(f\"k={k}: Inertia={kmeans.inertia_:.2f}, Silhouette Score={sil_score:.3f}\")\n",
    "\n",
    "# Plot Elbow Method and Silhouette Score\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Elbow Method Plot\n",
    "ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Inertia (Within-cluster Sum of Squares)')\n",
    "ax1.set_title('Elbow Method for Optimal k', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations for key points\n",
    "for i, (k, inertia) in enumerate(zip(k_range, inertias)):\n",
    "    ax1.annotate(f'{inertia:.1f}', (k, inertia), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center', fontsize=10)\n",
    "\n",
    "# Silhouette Score Plot\n",
    "ax2.plot(k_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Number of Clusters (k)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Score for Different k', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations for silhouette scores\n",
    "for i, (k, score) in enumerate(zip(k_range, silhouette_scores)):\n",
    "    ax2.annotate(f'{score:.3f}', (k, score), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal k\n",
    "optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
    "max_silhouette = max(silhouette_scores)\n",
    "\n",
    "print(f\"\\\\nüéØ OPTIMAL NUMBER OF CLUSTERS:\")\n",
    "print(f\"Based on Silhouette Score: k = {optimal_k_silhouette} (Score: {max_silhouette:.3f})\")\n",
    "\n",
    "# Calculate elbow point (simple method - looking for the point where the rate of decrease slows)\n",
    "# Calculate the differences\n",
    "diffs = [inertias[i] - inertias[i+1] for i in range(len(inertias)-1)]\n",
    "# Find where the difference starts to level off\n",
    "elbow_k = k_range[np.argmax(np.diff(diffs))] + 2  # Add 2 to adjust for indexing\n",
    "print(f\"Based on Elbow Method: k ‚âà {elbow_k}\")\n",
    "\n",
    "# Use the silhouette score optimal k for final clustering\n",
    "final_k = optimal_k_silhouette\n",
    "print(f\"\\\\n‚úÖ Selected k = {final_k} for final clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4298e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans Clustering Implementation\n",
    "print(\"üéØ K-MEANS CLUSTERING IMPLEMENTATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize and fit KMeans with optimal k\n",
    "kmeans_final = KMeans(n_clusters=final_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to the original data\n",
    "cluster_data_clean['Cluster'] = cluster_labels\n",
    "\n",
    "print(f\"‚úÖ KMeans clustering completed with k={final_k}\")\n",
    "print(f\"Final silhouette score: {silhouette_score(X_scaled, cluster_labels):.3f}\")\n",
    "\n",
    "# Cluster distribution\n",
    "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "print(f\"\\\\nüìä CLUSTER DISTRIBUTION:\")\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    print(f\"Cluster {cluster_id}: {count} states ({count/len(cluster_labels)*100:.1f}%)\")\n",
    "\n",
    "# Display states in each cluster\n",
    "print(f\"\\\\nüèõÔ∏è STATES BY CLUSTER:\")\n",
    "for cluster_id in sorted(cluster_data_clean['Cluster'].unique()):\n",
    "    states_in_cluster = cluster_data_clean[cluster_data_clean['Cluster'] == cluster_id]['State_Name'].tolist()\n",
    "    print(f\"\\\\nCluster {cluster_id} ({len(states_in_cluster)} states):\")\n",
    "    print(\", \".join(states_in_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a2cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis for Visualization\n",
    "print(\"üìä PRINCIPAL COMPONENT ANALYSIS FOR VISUALIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Apply PCA for 2D visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"‚úÖ PCA completed\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {pca.explained_variance_ratio_.sum():.3f} ({pca.explained_variance_ratio_.sum()*100:.1f}%)\")\n",
    "\n",
    "# Create PCA DataFrame\n",
    "pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'], index=cluster_data_clean.index)\n",
    "pca_df['Cluster'] = cluster_labels\n",
    "pca_df['State_Name'] = cluster_data_clean['State_Name']\n",
    "\n",
    "# Visualize clusters in 2D PCA space\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Create scatter plot\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, final_k))\n",
    "for i in range(final_k):\n",
    "    cluster_mask = pca_df['Cluster'] == i\n",
    "    plt.scatter(pca_df[cluster_mask]['PC1'], \n",
    "               pca_df[cluster_mask]['PC2'],\n",
    "               c=[colors[i]], \n",
    "               label=f'Cluster {i}', \n",
    "               s=100, \n",
    "               alpha=0.7,\n",
    "               edgecolors='black',\n",
    "               linewidth=0.5)\n",
    "\n",
    "# Add state labels to points\n",
    "for idx, row in pca_df.iterrows():\n",
    "    plt.annotate(row['State_Name'], \n",
    "                (row['PC1'], row['PC2']),\n",
    "                xytext=(5, 5), \n",
    "                textcoords='offset points',\n",
    "                fontsize=9,\n",
    "                alpha=0.8)\n",
    "\n",
    "# Add cluster centers\n",
    "centers_pca = pca.transform(kmeans_final.cluster_centers_)\n",
    "plt.scatter(centers_pca[:, 0], centers_pca[:, 1], \n",
    "           c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "\n",
    "plt.xlabel(f'First Principal Component ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'Second Principal Component ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.title('K-Means Clustering Results - PCA Visualization\\\\n(India Census 2011 State-level Data)', \n",
    "         fontweight='bold', fontsize=14)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print PCA component interpretation\n",
    "print(f\"\\\\nüîç PCA COMPONENTS INTERPRETATION:\")\n",
    "feature_names = X.columns\n",
    "components_df = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=feature_names\n",
    ")\n",
    "\n",
    "print(\"\\\\nComponent loadings (how much each feature contributes to each component):\")\n",
    "print(components_df.round(3))\n",
    "\n",
    "# Find features with highest absolute loadings for each component\n",
    "print(\"\\\\nüìà MAIN DRIVERS OF EACH COMPONENT:\")\n",
    "for i, pc in enumerate(['PC1', 'PC2']):\n",
    "    loadings = components_df[pc].abs().sort_values(ascending=False)\n",
    "    print(f\"\\\\n{pc} (explains {pca.explained_variance_ratio_[i]:.1%} of variance):\")\n",
    "    for j, (feature, loading) in enumerate(loadings.head(3).items()):\n",
    "        direction = \"+\" if components_df.loc[feature, pc] > 0 else \"-\"\n",
    "        print(f\"  {j+1}. {feature}: {direction}{loading:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76217497",
   "metadata": {},
   "source": [
    "## 7. Cluster Analysis and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afcefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Characteristics Analysis\n",
    "print(\"üìä CLUSTER CHARACTERISTICS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate mean values for each cluster\n",
    "cluster_means = cluster_data_clean.groupby('Cluster')[available_features].mean()\n",
    "\n",
    "print(\"üìà CLUSTER MEAN VALUES:\")\n",
    "print(cluster_means.round(2))\n",
    "\n",
    "# Create a comprehensive comparison table\n",
    "cluster_summary = []\n",
    "for cluster_id in sorted(cluster_data_clean['Cluster'].unique()):\n",
    "    cluster_subset = cluster_data_clean[cluster_data_clean['Cluster'] == cluster_id]\n",
    "    \n",
    "    summary = {\n",
    "        'Cluster': cluster_id,\n",
    "        'States_Count': len(cluster_subset),\n",
    "        'Avg_Literacy_Rate': cluster_subset['Literacy_Rate'].mean(),\n",
    "        'Avg_Sex_Ratio': cluster_subset['Sex_Ratio'].mean(),\n",
    "        'Avg_Work_Participation': cluster_subset['Work_Participation_Rate'].mean(),\n",
    "        'Avg_Child_Population': cluster_subset['Child_Population_Ratio'].mean(),\n",
    "        'Avg_SC_Population': cluster_subset['SC_Population_Percent'].mean(),\n",
    "        'Avg_ST_Population': cluster_subset['ST_Population_Percent'].mean(),\n",
    "        'Avg_Population': cluster_subset['TOT_P'].mean()\n",
    "    }\n",
    "    cluster_summary.append(summary)\n",
    "\n",
    "summary_df = pd.DataFrame(cluster_summary)\n",
    "print(f\"\\\\nüìã CLUSTER SUMMARY TABLE:\")\n",
    "print(summary_df.round(2))\n",
    "\n",
    "# Visualize cluster characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Literacy Rate by Cluster\n",
    "ax1 = axes[0, 0]\n",
    "cluster_means['Literacy_Rate'].plot(kind='bar', ax=ax1, color='skyblue', alpha=0.8)\n",
    "ax1.set_title('Average Literacy Rate by Cluster', fontweight='bold')\n",
    "ax1.set_ylabel('Literacy Rate (%)')\n",
    "ax1.set_xlabel('Cluster')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Sex Ratio by Cluster\n",
    "ax2 = axes[0, 1]\n",
    "cluster_means['Sex_Ratio'].plot(kind='bar', ax=ax2, color='lightcoral', alpha=0.8)\n",
    "ax2.set_title('Average Sex Ratio by Cluster', fontweight='bold')\n",
    "ax2.set_ylabel('Sex Ratio (F per 1000 M)')\n",
    "ax2.set_xlabel('Cluster')\n",
    "ax2.tick_params(axis='x', rotation=0)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Work Participation by Cluster\n",
    "ax3 = axes[1, 0]\n",
    "cluster_means['Work_Participation_Rate'].plot(kind='bar', ax=ax3, color='lightgreen', alpha=0.8)\n",
    "ax3.set_title('Average Work Participation Rate by Cluster', fontweight='bold')\n",
    "ax3.set_ylabel('Work Participation (%)')\n",
    "ax3.set_xlabel('Cluster')\n",
    "ax3.tick_params(axis='x', rotation=0)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Population Size by Cluster\n",
    "ax4 = axes[1, 1]\n",
    "cluster_means['TOT_P'].plot(kind='bar', ax=ax4, color='orange', alpha=0.8)\n",
    "ax4.set_title('Average Population Size by Cluster', fontweight='bold')\n",
    "ax4.set_ylabel('Population (in millions)')\n",
    "ax4.set_xlabel('Cluster')\n",
    "ax4.tick_params(axis='x', rotation=0)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Format population values to millions\n",
    "ax4.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Cluster Characteristics Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Cluster Interpretation\n",
    "print(\"üîç DETAILED CLUSTER INTERPRETATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze each cluster in detail\n",
    "for cluster_id in sorted(cluster_data_clean['Cluster'].unique()):\n",
    "    cluster_subset = cluster_data_clean[cluster_data_clean['Cluster'] == cluster_id]\n",
    "    \n",
    "    print(f\"\\\\n{'='*20} CLUSTER {cluster_id} {'='*20}\")\n",
    "    print(f\"Number of States: {len(cluster_subset)}\")\n",
    "    print(f\"States: {', '.join(cluster_subset['State_Name'].tolist())}\")\n",
    "    \n",
    "    # Key characteristics\n",
    "    print(f\"\\\\nüìä Key Characteristics:\")\n",
    "    print(f\"  ‚Ä¢ Literacy Rate: {cluster_subset['Literacy_Rate'].mean():.1f}% (Range: {cluster_subset['Literacy_Rate'].min():.1f}% - {cluster_subset['Literacy_Rate'].max():.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Sex Ratio: {cluster_subset['Sex_Ratio'].mean():.0f} (Range: {cluster_subset['Sex_Ratio'].min():.0f} - {cluster_subset['Sex_Ratio'].max():.0f})\")\n",
    "    print(f\"  ‚Ä¢ Work Participation: {cluster_subset['Work_Participation_Rate'].mean():.1f}% (Range: {cluster_subset['Work_Participation_Rate'].min():.1f}% - {cluster_subset['Work_Participation_Rate'].max():.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Child Population: {cluster_subset['Child_Population_Ratio'].mean():.1f}% (Range: {cluster_subset['Child_Population_Ratio'].min():.1f}% - {cluster_subset['Child_Population_Ratio'].max():.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ SC Population: {cluster_subset['SC_Population_Percent'].mean():.1f}% (Range: {cluster_subset['SC_Population_Percent'].min():.1f}% - {cluster_subset['SC_Population_Percent'].max():.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ ST Population: {cluster_subset['ST_Population_Percent'].mean():.1f}% (Range: {cluster_subset['ST_Population_Percent'].min():.1f}% - {cluster_subset['ST_Population_Percent'].max():.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Average Population: {cluster_subset['TOT_P'].mean()/1e6:.1f} million\")\n",
    "    \n",
    "    if 'Gender_Literacy_Gap' in cluster_subset.columns:\n",
    "        print(f\"  ‚Ä¢ Gender Literacy Gap: {cluster_subset['Gender_Literacy_Gap'].mean():.1f} percentage points\")\n",
    "\n",
    "# Provide interpretive labels for clusters based on characteristics\n",
    "print(f\"\\\\nüè∑Ô∏è CLUSTER INTERPRETATION & LABELING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cluster_interpretations = {}\n",
    "\n",
    "for cluster_id in sorted(cluster_data_clean['Cluster'].unique()):\n",
    "    cluster_subset = cluster_data_clean[cluster_data_clean['Cluster'] == cluster_id]\n",
    "    \n",
    "    # Determine cluster characteristics\n",
    "    literacy = cluster_subset['Literacy_Rate'].mean()\n",
    "    sex_ratio = cluster_subset['Sex_Ratio'].mean()\n",
    "    work_participation = cluster_subset['Work_Participation_Rate'].mean()\n",
    "    st_population = cluster_subset['ST_Population_Percent'].mean()\n",
    "    \n",
    "    # Generate interpretation based on characteristics\n",
    "    if literacy > 85:\n",
    "        literacy_label = \"High Literacy\"\n",
    "    elif literacy > 70:\n",
    "        literacy_label = \"Moderate Literacy\"\n",
    "    else:\n",
    "        literacy_label = \"Low Literacy\"\n",
    "    \n",
    "    if sex_ratio > 950:\n",
    "        gender_label = \"Good Gender Balance\"\n",
    "    elif sex_ratio > 900:\n",
    "        gender_label = \"Moderate Gender Balance\"\n",
    "    else:\n",
    "        gender_label = \"Poor Gender Balance\"\n",
    "    \n",
    "    if work_participation > 45:\n",
    "        work_label = \"High Work Participation\"\n",
    "    elif work_participation > 35:\n",
    "        work_label = \"Moderate Work Participation\"\n",
    "    else:\n",
    "        work_label = \"Low Work Participation\"\n",
    "    \n",
    "    if st_population > 20:\n",
    "        tribal_label = \"High Tribal Population\"\n",
    "    elif st_population > 10:\n",
    "        tribal_label = \"Moderate Tribal Population\"\n",
    "    else:\n",
    "        tribal_label = \"Low Tribal Population\"\n",
    "    \n",
    "    # Combine characteristics for cluster label\n",
    "    interpretation = f\"{literacy_label}, {work_label}, {tribal_label}\"\n",
    "    cluster_interpretations[cluster_id] = interpretation\n",
    "    \n",
    "    print(f\"\\\\nCluster {cluster_id}: {interpretation}\")\n",
    "    print(f\"  Profile: {literacy_label} ({literacy:.1f}%), {work_label} ({work_participation:.1f}%), {tribal_label} ({st_population:.1f}%)\")\n",
    "\n",
    "# Add interpretative labels to the data\n",
    "cluster_data_clean['Cluster_Label'] = cluster_data_clean['Cluster'].map(cluster_interpretations)\n",
    "\n",
    "print(f\"\\\\n‚úÖ Cluster analysis completed!\")\n",
    "print(f\"Summary: {len(cluster_data_clean)} states grouped into {final_k} distinct clusters based on demographic characteristics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2539dca7",
   "metadata": {},
   "source": [
    "## 8. Save Processed Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b844ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data and models\n",
    "print(\"üíæ SAVING PROCESSED DATA AND MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create outputs directory if it doesn't exist\n",
    "output_dir = \"../outputs\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"‚úÖ Created output directory: {output_dir}\")\n",
    "\n",
    "# 1. Save cleaned dataset with features\n",
    "cleaned_data_path = os.path.join(output_dir, \"cleaned_dataset.csv\")\n",
    "df_state_features.to_csv(cleaned_data_path, index=False)\n",
    "print(f\"‚úÖ Saved cleaned dataset: {cleaned_data_path}\")\n",
    "\n",
    "# 2. Save cluster results\n",
    "cluster_results_path = os.path.join(output_dir, \"cluster_results.csv\")\n",
    "cluster_data_clean.to_csv(cluster_results_path, index=False)\n",
    "print(f\"‚úÖ Saved cluster results: {cluster_results_path}\")\n",
    "\n",
    "# 3. Save the trained KMeans model\n",
    "model_path = os.path.join(output_dir, \"kmeans_model.pkl\")\n",
    "joblib.dump(kmeans_final, model_path)\n",
    "print(f\"‚úÖ Saved KMeans model: {model_path}\")\n",
    "\n",
    "# 4. Save the scaler\n",
    "scaler_path = os.path.join(output_dir, \"scaler.pkl\")\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"‚úÖ Saved StandardScaler: {scaler_path}\")\n",
    "\n",
    "# 5. Save PCA model\n",
    "pca_path = os.path.join(output_dir, \"pca_model.pkl\")\n",
    "joblib.dump(pca, pca_path)\n",
    "print(f\"‚úÖ Saved PCA model: {pca_path}\")\n",
    "\n",
    "# 6. Save feature names used in clustering\n",
    "features_path = os.path.join(output_dir, \"feature_names.txt\")\n",
    "with open(features_path, 'w') as f:\n",
    "    for feature in available_features:\n",
    "        f.write(f\"{feature}\\\\n\")\n",
    "print(f\"‚úÖ Saved feature names: {features_path}\")\n",
    "\n",
    "# 7. Create a summary report\n",
    "summary_path = os.path.join(output_dir, \"analysis_summary.txt\")\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"INDIA CENSUS 2011 EDA & CLUSTERING ANALYSIS SUMMARY\\\\n\")\n",
    "    f.write(\"=\"*60 + \"\\\\n\\\\n\")\n",
    "    f.write(f\"Dataset: India Census 2011\\\\n\")\n",
    "    f.write(f\"Analysis Level: State-level data\\\\n\")\n",
    "    f.write(f\"Total States Analyzed: {len(cluster_data_clean)}\\\\n\")\n",
    "    f.write(f\"Features Used for Clustering: {len(available_features)}\\\\n\")\n",
    "    f.write(f\"Optimal Number of Clusters: {final_k}\\\\n\")\n",
    "    f.write(f\"Final Silhouette Score: {silhouette_score(X_scaled, cluster_labels):.3f}\\\\n\\\\n\")\n",
    "    \n",
    "    f.write(\"CLUSTER INTERPRETATIONS:\\\\n\")\n",
    "    f.write(\"-\" * 30 + \"\\\\n\")\n",
    "    for cluster_id, interpretation in cluster_interpretations.items():\n",
    "        states_in_cluster = cluster_data_clean[cluster_data_clean['Cluster'] == cluster_id]['State_Name'].tolist()\n",
    "        f.write(f\"\\\\nCluster {cluster_id}: {interpretation}\\\\n\")\n",
    "        f.write(f\"States ({len(states_in_cluster)}): {', '.join(states_in_cluster)}\\\\n\")\n",
    "    \n",
    "    f.write(f\"\\\\n\\\\nFEATURES USED:\\\\n\")\n",
    "    f.write(\"-\" * 15 + \"\\\\n\")\n",
    "    for i, feature in enumerate(available_features, 1):\n",
    "        f.write(f\"{i}. {feature}\\\\n\")\n",
    "\n",
    "print(f\"‚úÖ Saved analysis summary: {summary_path}\")\n",
    "\n",
    "# Display saved files\n",
    "print(f\"\\\\nüìÅ FILES SAVED TO {output_dir}:\")\n",
    "saved_files = [\n",
    "    \"cleaned_dataset.csv\",\n",
    "    \"cluster_results.csv\", \n",
    "    \"kmeans_model.pkl\",\n",
    "    \"scaler.pkl\",\n",
    "    \"pca_model.pkl\",\n",
    "    \"feature_names.txt\",\n",
    "    \"analysis_summary.txt\"m \n",
    "]\n",
    "\n",
    "for file in saved_files:\n",
    "    file_path = os.path.join(output_dir, file)\n",
    "    if os.path.exists(file_path):\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        print(f\"  ‚úì {file} ({file_size:,} bytes)\")\n",
    "\n",
    "print(f\"\\\\nüéâ Analysis completed successfully!\")\n",
    "print(f\"üìä {len(cluster_data_clean)} states clustered into {final_k} groups\")\n",
    "print(f\"üíæ All results saved to {output_dir}\")\n",
    "print(f\"\\\\nüìã Next steps:\")\n",
    "print(f\"  1. Review cluster results in cluster_results.csv\")\n",
    "print(f\"  2. Use saved models for predictions on new data\")\n",
    "print(f\"  3. Run the Streamlit dashboard for interactive visualization\")\n",
    "print(f\"  4. Command: streamlit run ../dashboard/app.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
